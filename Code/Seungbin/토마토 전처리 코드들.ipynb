{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a217850b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model, Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Input, Concatenate, concatenate,Dropout\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D,GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm.keras import TqdmCallback\n",
    "import tensorflow.keras.utils as utils\n",
    "from datetime import datetime\n",
    "\n",
    "import cv2\n",
    "import zipfile\n",
    "import shutil\n",
    "import os\n",
    "import glob, json\n",
    "\n",
    "from tensorflow.keras.applications import Xception, InceptionV3, MobileNet, ResNet50\n",
    "\n",
    "import random\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f6bce4",
   "metadata": {},
   "source": [
    "# Json을 Txt로 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0703cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Dictionary mapping type to class id\n",
    "type_to_id = { 'd1': 2, 'd2': 2, 'f': 3, 'e': 4}\n",
    "\n",
    "# Input directories and output directory\n",
    "input_dirs = ['d1.꽃(개화군)', 'd2.꽃(착과군)', 'e.열매', 'f.만개꽃']\n",
    "output_dir = 'D:\\\\097.지능형 스마트팜 통합 데이터(토마토)\\\\01.데이터\\\\text'\n",
    "base_dir = 'D:\\\\097.지능형 스마트팜 통합 데이터(토마토)\\\\01.데이터\\\\1.Training\\\\라벨링데이터'\n",
    "\n",
    "# Function to convert polygon to bounding box\n",
    "def poly_to_bbox(poly_points):\n",
    "    xs = [point[0] for point in poly_points]\n",
    "    ys = [point[1] for point in poly_points]\n",
    "    \n",
    "    x1 = min(xs)\n",
    "    y1 = min(ys)\n",
    "    x2 = max(xs)\n",
    "    y2 = max(ys)\n",
    "    \n",
    "    return [[x1, y1], [x2, y2]]\n",
    "\n",
    "# Loop over input directories\n",
    "for input_dir in tqdm(input_dirs, desc=\"Processing directories\"):\n",
    "    for filename in os.listdir(os.path.join(base_dir, input_dir)):\n",
    "        if filename.endswith('.json'):\n",
    "            # Open JSON file\n",
    "            with open(os.path.join(base_dir, input_dir, filename)) as f:\n",
    "                data = json.load(f)\n",
    "                \n",
    "                # Create directory if not exists\n",
    "                os.makedirs(os.path.join(output_dir, input_dir), exist_ok=True)\n",
    "\n",
    "                # Create output .txt file\n",
    "                with open(os.path.join(output_dir, input_dir, os.path.splitext(filename)[0] + '.txt'), 'w') as outfile:\n",
    "                    # Loop over shapes in data\n",
    "                    for shape in data['shapes']:\n",
    "                        # Get class id from type\n",
    "                        class_id = type_to_id[data['file_attributes'][\"type\"]]\n",
    "                        \n",
    "                        # Convert polygon to bounding box\n",
    "                        [x1, y1], [x2, y2] = poly_to_bbox(shape['points'])\n",
    "\n",
    "                        # Calculate YOLO format values\n",
    "                        x_center = (x1 + x2) / 2.0 / data['imageWidth']\n",
    "                        y_center = (y1 + y2) / 2.0 / data['imageHeight']\n",
    "                        width = abs(x2 - x1) / data['imageWidth']\n",
    "                        height = abs(y2 - y1) / data['imageHeight']\n",
    "\n",
    "                        # Write to output file\n",
    "                        outfile.write(f'{class_id} {x_center} {y_center} {width} {height}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0706459",
   "metadata": {},
   "source": [
    "# 이름 순서 바꾸기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fa458d",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"D:\\\\097.지능형 스마트팜 통합 데이터(토마토)\\\\01.데이터\"\n",
    "\n",
    "def rearrange_filename(filename):\n",
    "    # Split the filename into parts\n",
    "    parts = filename.split('_')\n",
    "\n",
    "    # Rearrange the parts of the filename as per the new order\n",
    "    rearranged = [parts[0], parts[1], parts[3],parts[5], parts[2], parts[4]] + parts[6:]\n",
    "\n",
    "    # Join the parts back together and return the result\n",
    "    return '_'.join(rearranged)\n",
    "\n",
    "# Pre-calculate total iterations\n",
    "total_iterations = 0\n",
    "for index_1 in os.listdir(root_dir):\n",
    "    for index_2 in os.listdir(os.path.join(root_dir,index_1)):\n",
    "        for index_3 in os.listdir(os.path.join(root_dir,index_1,index_2)):\n",
    "            folder_path = os.path.join(root_dir,index_1,index_2,index_3)\n",
    "            if os.path.isdir(folder_path):\n",
    "                folder_cat, file_extension = os.path.splitext(folder_path)\n",
    "                if file_extension != \".zip\":\n",
    "                    if os.path.basename(folder_path) != \"TS_Timeseries\":\n",
    "                        total_iterations += len(os.listdir(folder_path))\n",
    "\n",
    "with tqdm(total=total_iterations) as pbar:\n",
    "    for index_1 in os.listdir(root_dir):\n",
    "        for index_2 in os.listdir(os.path.join(root_dir,index_1)):\n",
    "            for index_3 in os.listdir(os.path.join(root_dir,index_1,index_2)):\n",
    "                folder_path = os.path.join(root_dir,index_1,index_2,index_3)\n",
    "\n",
    "                # Ensure it's actually a folder, not a file\n",
    "                if os.path.isdir(folder_path):\n",
    "                    folder_cat, file_extension = os.path.splitext(folder_path)\n",
    "                    if file_extension != \".zip\":\n",
    "                        if os.path.basename(folder_path) != \"TS_Timeseries\":\n",
    "                            for file_name in os.listdir(folder_path):\n",
    "                                old_filepath = os.path.join(folder_path, file_name)\n",
    "                                # Ensure it's actually a file\n",
    "                                if os.path.isfile(old_filepath):\n",
    "                                    # Rearrange the filename\n",
    "                                    new_filename = rearrange_filename(file_name)\n",
    "                                    new_filepath = os.path.join(folder_path, new_filename)\n",
    "\n",
    "                                    # Rename the file\n",
    "                                    os.rename(old_filepath, new_filepath)\n",
    "                                    pbar.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f227a6a",
   "metadata": {},
   "source": [
    "# 2번 농장 환경정보 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed9157f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path\n",
    "tom2 = \"D:\\\\097.지능형 스마트팜 통합 데이터(토마토)\\\\01.데이터\\\\1.Training\\\\원천데이터\\\\TS_Timeseries\\\\tom2\\\\\"\n",
    "\n",
    "#파일 목록보기\n",
    "print(os.listdir(os.path.join(tom2,\"양액기 수집정보\")))\n",
    "\n",
    "# Define a function to detect the encoding of a file\n",
    "def detect_encoding(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        result = chardet.detect(f.read())\n",
    "    return result['encoding']\n",
    "# Pandas로 csv를 호출할 때 csv 인코딩 정보를 사용하도록 해당 csv파일의 인코딩 정보를 추출하는 함수\n",
    "# 해당 csv는 ANSI, UTF-8(BOM), UTF-8이 혼재. \n",
    "\n",
    "# Get the list of file names\n",
    "tom2_file_list = glob.glob(os.path.join(tom2, \"양액기 수집정보\", \"*\"))\n",
    "\n",
    "# Create an empty dictionary to hold the DataFrames\n",
    "lp = {}\n",
    "\n",
    "# Loop over all files in the directory\n",
    "for file_path in tom2_file_list:\n",
    "    # Detect the encoding of the file\n",
    "    encoding = detect_encoding(file_path)\n",
    "\n",
    "    # Open the file with the detected encoding\n",
    "    with open(file_path, 'r', encoding=encoding) as f:\n",
    "        # Read the first line and split it to get column names\n",
    "        columns = f.readline().strip().split(';')\n",
    "\n",
    "        # Clean up the column names by replacing double quotes\n",
    "        columns = [col.replace('\"', '') for col in columns]\n",
    "        columns = [col.strip() for col in columns]\n",
    "\n",
    "        # Read the rest of the file into a DataFrame\n",
    "        df = pd.read_csv(f, sep=';', names=columns, header=None)\n",
    "\n",
    "    # Remove double quotes from data\n",
    "    df = df.replace({'\"': ''}, regex=True)\n",
    "\n",
    "    # Save the DataFrame in the dictionary\n",
    "    # Using the filename (without extension) as the key\n",
    "    file_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    lp[file_name] = df\n",
    "    \n",
    "# Time column 변경\n",
    "for key in lp.keys():\n",
    "    lp[key][\"Time\"] = pd.to_datetime(lp[key][\"Time\"])\n",
    "    lp[key][\"Time\"] = lp[key][\"Time\"].dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "# MERGE\n",
    "# Get a sorted list of the keys (optional)\n",
    "keys = sorted(list(lp.keys()))\n",
    "\n",
    "# Use the DataFrame from the first CSV file as the starting point\n",
    "merged = lp[keys[0]]\n",
    "\n",
    "# Loop over all other DataFrames and merge them into the merged DataFrame\n",
    "for key in keys[1:]:\n",
    "    merged = pd.merge(merged, lp[key], on='Time', how='inner')\n",
    "\n",
    "print(merged.head(3))\n",
    "\n",
    "# NULL 확인\n",
    "for col in merged.columns:\n",
    "    print(f\"{col}'s null : {merged[col].isnull().sum()}\")\n",
    "\n",
    "# NULL 전처리를 위해 NULL이 있는 row의 앞뒤 값을 확인\n",
    "null_indices = lp[\"MC_temp\"][lp[\"MC_temp\"][\"Temp_Soil\"].isnull()].index\n",
    "\n",
    "# Expand your indices to include the preceding and following rows\n",
    "indices = []\n",
    "for index in null_indices:\n",
    "    indices.extend([index-1, index, index+1])\n",
    "\n",
    "# 해당 NULL을 앞 뒤 5개씩 총 10개의 함수의 평균으로 채우기\n",
    "from IPython.display import display\n",
    "pd.set_option('display.max_rows', None)  # or any number you prefer\n",
    "pd.set_option('display.max_columns', None)  # or any number you prefer\n",
    "\n",
    "surrounding_rows = lp[\"MC_temp\"].loc[indices]\n",
    "print(surrounding_rows)\n",
    "\n",
    "pd.reset_option('display.max_rows')\n",
    "pd.reset_option('display.max_columns')\n",
    "\n",
    "def fill_with_mean(df, column):\n",
    "    null_indices = df[df[column].isnull()].index\n",
    "    for idx in null_indices:\n",
    "        # Check if the previous and next indices are in the dataframe to avoid errors\n",
    "        if idx - 1 in df.index and idx + 5 in df.index:\n",
    "            before_value = df.loc[idx - 5, column]\n",
    "            after_value = df.loc[idx + 5, column]\n",
    "            \n",
    "            # Only fill the NaN if both before and after values are not NaN\n",
    "            if pd.notnull(before_value) and pd.notnull(after_value):\n",
    "                df.loc[idx, column] = (before_value + after_value) / 5.0\n",
    "    return df\n",
    "\n",
    "lp[\"MC_temp\"] = fill_with_mean(lp[\"MC_temp\"], \"Temp_Soil\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cad5cb",
   "metadata": {},
   "source": [
    "# txt 제목 바꾸기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e11a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat in tqdm(os.listdir(root_dir)):\n",
    "    for file_name in os.listdir(os.path.join(root_dir, cat)):\n",
    "        old_path = os.path.join(root_dir, cat, file_name)\n",
    "        old_name = os.path.basename(old_path)\n",
    "        parts = old_name.split(\"_\")\n",
    "\n",
    "        rearranged = [parts[0], parts[1], parts[4], parts[2], parts[5], parts[3]]\n",
    "        rearranged.extend(parts[6:])\n",
    "        \n",
    "        new_name = \"_\".join(rearranged)\n",
    "        \n",
    "        # ensure the extension is preserved\n",
    "        new_name += os.path.splitext(old_path)[1]\n",
    "\n",
    "        new_path = os.path.join(root_dir, cat, new_name)\n",
    "\n",
    "        # rename the file\n",
    "        os.rename(old_path, new_path)\n",
    "        \n",
    "        \n",
    "# 바꿀 순서 확인\n",
    "original_name=\"V001_tom1_39_001_d1_09_20210930_14_03135225_49122255.png\"\n",
    "target_name=\"V001_tom1_001_09_39_d1_20210930_14_03135225_49122255.txt\"\n",
    "\n",
    "parts=target_name.split(\"_\")\n",
    "\n",
    "print(\"Original Name: \", original_name)\n",
    "part=[parts[0],parts[1],parts[4],parts[2],parts[5],parts[3]]\n",
    "part.extend(parts[6:])\n",
    "reassembled_name = \"_\".join(part)\n",
    "print(\"Reassembled Name: \", reassembled_name)\n",
    "print(type(reassembled_name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
